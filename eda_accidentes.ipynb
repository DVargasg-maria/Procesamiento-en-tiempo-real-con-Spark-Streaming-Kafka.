# ----------- Instalar y configurar (ejecutar una vez) -------------
!pip install -q pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, trim, when, regexp_replace, to_date, substring, instr, split, size,
    lpad, concat_ws, to_timestamp, date_format, hour, dayofweek, month,
    count, desc, lit
)

# Crear sesión Spark
spark = SparkSession.builder.appName("AccidentesBogota").getOrCreate()

# ----------- 1) Leer CSV (asegúrate de poner el nombre correcto del archivo) -------------
df = spark.read.option("header", True).option("sep", ";").csv("NUMERODESINIESTROS.csv")
print("Lectura completa. Columnas originales:", df.columns)
df.show(3, truncate=False)

# ----------- 2) Normalizar nombres de columna (sin espacios y en mayúsculas) -------------
new_cols = [c.strip().upper() for c in df.columns]
df = df.toDF(*new_cols)
print("Columnas normalizadas:", df.columns)

# ----------- 3) Limpiar valores generales: trim y normalizar 'NULL' / '' -------------
for c in df.columns:
    df = df.withColumn(c, when(trim(col(c)) == "", None).otherwise(trim(col(c))))
    df = df.withColumn(c, when(col(c).isin("NULL","null","Null"), None).otherwise(col(c)))

# ----------- 4) LATITUD / LONGITUD: reemplazar coma decimal y castear a double -------------
df = df.withColumn("LATITUD", regexp_replace(col("LATITUD"), ",", "."))
df = df.withColumn("LONGITUD", regexp_replace(col("LONGITUD"), ",", "."))
df = df.withColumn("LATITUD", col("LATITUD").cast("double"))
df = df.withColumn("LONGITUD", col("LONGITUD").cast("double"))

# Verificar
df.select("LATITUD", "LONGITUD").printSchema()
df.select("LATITUD", "LONGITUD").show(3, truncate=False)

# ----------- 5) NORMALIZAR FECHAS y HORAS: crear FECHA_HORA_TS -------------
# Backup de FECHA_HORA si existe
if "FECHA_HORA" in df.columns:
    df = df.withColumnRenamed("FECHA_HORA", "FECHA_HORA_ORIG")

# Asegurarnos de que existan las columnas FECHA_OCUR y HORA_OCURR (si no, crearlas nulas para no romper el flujo)
if "FECHA_OCUR" not in df.columns:
    df = df.withColumn("FECHA_OCUR", lit(None))
if "HORA_OCURR" not in df.columns:
    df = df.withColumn("HORA_OCURR", lit(None))

# Limpieza básica de HORA_OCURR (reemplazar '.' por ':')
df = df.withColumn("HORA_OCURR", regexp_replace(col("HORA_OCURR"), r"\.", ":"))
df = df.withColumn("HORA_OCURR", when(col("HORA_OCURR") == "", None).otherwise(col("HORA_OCURR")))

# Extraer solo fecha de FECHA_OCUR (intento general)
df = df.withColumn("FECHA_OCUR_SONLY", to_date(col("FECHA_OCUR")))
# Si to_date falla, extraer primeros 10 caracteres (ej: 'yyyy-MM-dd ...')
df = df.withColumn("FECHA_OCUR_SONLY",
                   when(col("FECHA_OCUR_SONLY").isNull() & col("FECHA_OCUR").isNotNull(),
                        to_date(substring(col("FECHA_OCUR"), 1, 10), "yyyy-MM-dd"))
                   .otherwise(col("FECHA_OCUR_SONLY")))

# Normalizar HORA_OCURR a formato HH:mm:ss (varios casos)
df = df.withColumn("HORA_OCURR_CLEAN",
                   when(col("HORA_OCURR").isNull(), None)
                   .when(instr(col("HORA_OCURR"), ":") == 0,
                         concat_ws(":", lpad(col("HORA_OCURR"), 2, "0"), lit("00"), lit("00")))
                   .when((instr(col("HORA_OCURR"), ":") > 0) & (size(split(col("HORA_OCURR"), ":")) == 2),
                         concat_ws(":",
                                   lpad(split(col("HORA_OCURR"), ":").getItem(0), 2, "0"),
                                   lpad(split(col("HORA_OCURR"), ":").getItem(1), 2, "0"),
                                   lit("00")))
                   .otherwise(col("HORA_OCURR"))
                  )

# Construir fecha string yyyy-MM-dd y concatenar con hora limpia
df = df.withColumn("FECHA_STR",
                   when(col("FECHA_OCUR_SONLY").isNotNull(), date_format(col("FECHA_OCUR_SONLY"), "yyyy-MM-dd"))
                   .otherwise(None))

df = df.withColumn("FECHA_HORA_COMBINADA_STR",
                   when((col("FECHA_STR").isNotNull()) & (col("HORA_OCURR_CLEAN").isNotNull()),
                        concat_ws(" ", col("FECHA_STR"), col("HORA_OCURR_CLEAN")))
                   .otherwise(None))

# Parsear a timestamp: priorizamos la combinada; si no existe, usamos FECHA_HORA_ORIG si está disponible
from pyspark.sql.types import TimestampType
if "FECHA_HORA_ORIG" in df.columns:
    df = df.withColumn("FECHA_HORA_TS",
                       when(col("FECHA_HORA_COMBINADA_STR").isNotNull(),
                            to_timestamp(col("FECHA_HORA_COMBINADA_STR"), "yyyy-MM-dd HH:mm:ss"))
                       .otherwise(to_timestamp(col("FECHA_HORA_ORIG"), "yyyy-MM-dd HH:mm:ss")))
else:
    df = df.withColumn("FECHA_HORA_TS",
                       when(col("FECHA_HORA_COMBINADA_STR").isNotNull(),
                            to_timestamp(col("FECHA_HORA_COMBINADA_STR"), "yyyy-MM-dd HH:mm:ss"))
                       .otherwise(lit(None).cast(TimestampType()))
                      )

# Mostrar cuántos quedaron sin timestamp y ejemplos si hay problemas
n_nulos = df.filter(col("FECHA_HORA_TS").isNull()).count()
print("Nulos en FECHA_HORA_TS:", n_nulos)
if n_nulos > 0:
    df.filter(col("FECHA_HORA_TS").isNull()).select("FECHA_OCUR","HORA_OCURR","FECHA_HORA_ORIG","FECHA_HORA_COMBINADA_STR").limit(10).show(truncate=False)

# Renombrar FECHA_OCUR_SONLY a FECHA_OCUR_DATE y eliminar auxiliares opcionales
df = df.withColumnRenamed("FECHA_OCUR_SONLY", "FECHA_OCUR_DATE")
cols_to_drop = [c for c in ["FECHA_HORA_COMBINADA_STR","FECHA_STR","FECHA_OCUR","HORA_OCURR","FECHA_HORA_ORIG"] if c in df.columns]
df = df.drop(*cols_to_drop)

# ----------- 6) Crear columnas útiles para EDA: HORA, DIA_SEMANA, MES -------------
df = df.withColumn("HORA", hour(col("FECHA_HORA_TS"))) \
       .withColumn("DIA_SEMANA", dayofweek(col("FECHA_HORA_TS"))) \
       .withColumn("MES", month(col("FECHA_HORA_TS")))

# ----------- 7) Filtrar coordenadas plausibles (opcional) -------------
df_clean = df.filter(
    (col("LATITUD").isNotNull()) & (col("LONGITUD").isNotNull()) &
    (col("LATITUD") > 4.0) & (col("LATITUD") < 5.1) &
    (col("LONGITUD") > -75.1) & (col("LONGITUD") < -73.0) &
    (col("LOCALIDAD").isNotNull()) &
    (col("FECHA_HORA_TS").isNotNull())
)

print("Registros iniciales:", df.count())
print("Registros tras limpieza y filtros:", df_clean.count())

# ----------- 8) EDA usando DataFrames -------------
# Accidentes por localidad
from pyspark.sql.functions import count, desc

accidentes_localidad = (
    df_clean.groupBy("LOCALIDAD")
            .agg(count("*").alias("TOTAL_ACCIDENTES"))
            .orderBy(desc("TOTAL_ACCIDENTES"))
)

accidentes_localidad.show(10, truncate=False)
# Las localidades con mayor número de accidentes en Bogotá son principalmente Kennedy, Engativá y Suba.
# Esto coincide con zonas de alto tráfico vehicular y densidad poblacional.

# Accidentes por tipo (CLASE_ACC)
accidentes_tipo = (
    df_clean.groupBy("CLASE_ACC")
            .agg(count("*").alias("TOTAL"))
            .orderBy(desc("TOTAL"))
)
accidentes_tipo.show(10, truncate=False)
# El tipo de accidente más frecuente es el choque, seguido del atropello.
# Esto sugiere la necesidad de mejorar la señalización y la educación vial.

# Accidentes por gravedad (GRAVEDAD)
accidentes_gravedad = (
    df_clean.groupBy("GRAVEDAD")
            .agg(count("*").alias("TOTAL"))
            .orderBy(desc("TOTAL"))
)
accidentes_gravedad.show(truncate=False)
# La mayoría de los siniestros son de tipo leve o con solo daños materiales, aunque los casos con heridos o fatales siguen siendo significativos.

# Accidentes por hora del día
accidentes_hora = (
    df_clean.groupBy("HORA")
            .agg(count("*").alias("TOTAL"))
            .orderBy("HORA")
)
accidentes_hora.show(24, truncate=False)
# Los picos de accidentalidad ocurren entre las 6 a.m. y 8 a.m., y nuevamente entre 5 p.m. y 7 p.m., coincidiendo con las horas pico de movilidad.

# Accidentes por mes
from pyspark.sql.functions import month

accidentes_mes = (
    df_clean.groupBy("MES")
            .agg(count("*").alias("TOTAL"))
            .orderBy("MES")
)
accidentes_mes.show(12, truncate=False)
# Los meses con mayor número de siniestros son noviembre y diciembre, posiblemente debido a la alta movilidad en temporada de fin de año.

# Tipo de accidente por localidad
df_clean.groupBy("LOCALIDAD", "CLASE_ACC") \
        .agg(count("*").alias("TOTAL")) \
        .orderBy(desc("TOTAL")) \
        .show(20, truncate=False)
# En Kennedy y Engativá predominan los choques, mientras que en zonas como Puente Aranda se observan más atropellos.

# ----------- 9) EDA usando RDDs -------------
# Contar accidentes por localidad
rdd_localidad = df_clean.select("LOCALIDAD").rdd.map(lambda x: (x.LOCALIDAD, 1))
rdd_result = rdd_localidad.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)
print("Top 10 localidades (RDD):")
for loc, total in rdd_result.take(10):
    print(f"{loc}: {total}")


# Contar accidentes fatales
rdd_fatales = df_clean.select("GRAVEDAD").rdd.filter(lambda x: x.GRAVEDAD == "FATAL")
print("Total accidentes fatales (RDD):", rdd_fatales.count())

# Contar accidentes por tipo (CLASE_ACC)
rdd_tipo = df_clean.select("CLASE_ACC").rdd.map(lambda x: (x.CLASE_ACC, 1))
rdd_tipo_result = rdd_tipo.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)
print("Tipos de accidente (RDD):")
for tipo, total in rdd_tipo_result.collect():
    print(f"{tipo}: {total}")

# ----------- 10) Almacenamiento de resultados procesados -------------
# Guardar DataFrame limpio completo
df_clean.write.mode("overwrite").parquet("/content/resultados/df_clean_parquet")

# Guardar resultados del EDA
accidentes_localidad.write.mode("overwrite").option("header", True).csv("/content/resultados/accidentes_localidad_csv")
